# lab1
## 前言
这里主要使用了各种并行工具实现向量加法。

由于向量加法是典型的内存密集型应用，其计算/访问内存比为0.5，
实际上在打开Release后
编译器进行自动向量化已经可以填满内存带宽，
多核/GPU并不能提高效率，还可能因为增加了内存拷贝开销导致时间增加，
因此预期大多数程序结果为负优化。

## baseline
选用串行算法作为baseline. 同时，书写了统一的接口方便测试与性能对比。

在common.h中，提前申请所需内存，并填充初值

为了减小误差，计时前运行一次程序作为warming up，随后多次运行向量加法程序，取运行时间均值。

为了避免编译器自动将计算过程优化掉，使用随机数随机取值，

最后，计算结果的hash值，确保算法的正确性。

## pthread
对于pthread，我选用了C++11标准封装好的thread标准库，使用modern C++代码
风格书写高性能代码, 在Debug模式下的对比得到了预期的优化效果：
code here
然而，由于Release下，自动优化的AVX256指令集已经可以填满内存带宽，因此效果不明显。

## openmp
在串行代码的基础上直接添加即可

## MPI
MPI主要用于多机间通信，其在单机上的overhead极大。
加之本应用为通信密集型，因此不论在Release还是Debug模式下，预期应为负优化。


## CUDA 

